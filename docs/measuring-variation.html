<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Measuring variation | A Compact Guide to Classical Inference</title>
  <meta name="description" content="Chapter 3 Measuring variation | A Compact Guide to Classical Inference" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Measuring variation | A Compact Guide to Classical Inference" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Measuring variation | A Compact Guide to Classical Inference" />
  
  
  

<meta name="author" content="Daniel Kaplan" />


<meta name="date" content="2019-12-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-and-variables.html"/>
<link rel="next" href="modeling-variation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Compact Guide to Classical Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html"><i class="fa fa-check"></i><b>1</b> What is classical inference?</a><ul>
<li class="chapter" data-level="" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html#and-why-should-i-read-this-book"><i class="fa fa-check"></i>… and why should I read this book?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-and-variables.html"><a href="data-and-variables.html"><i class="fa fa-check"></i><b>2</b> Data and variables</a><ul>
<li class="chapter" data-level="2.1" data-path="data-and-variables.html"><a href="data-and-variables.html#data-frames"><i class="fa fa-check"></i><b>2.1</b> Data frames</a></li>
<li class="chapter" data-level="2.2" data-path="data-and-variables.html"><a href="data-and-variables.html#tabulations"><i class="fa fa-check"></i><b>2.2</b> Tabulations</a></li>
<li class="chapter" data-level="2.3" data-path="data-and-variables.html"><a href="data-and-variables.html#quantitative-and-categorical-variables"><i class="fa fa-check"></i><b>2.3</b> Quantitative and categorical variables</a></li>
<li class="chapter" data-level="2.4" data-path="data-and-variables.html"><a href="data-and-variables.html#response-and-explanatory-variables"><i class="fa fa-check"></i><b>2.4</b> Response and explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="measuring-variation.html"><a href="measuring-variation.html"><i class="fa fa-check"></i><b>3</b> Measuring variation</a><ul>
<li class="chapter" data-level="3.1" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-numerical-variable"><i class="fa fa-check"></i><b>3.1</b> Variance of a numerical variable</a></li>
<li class="chapter" data-level="3.2" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-categorical-variable"><i class="fa fa-check"></i><b>3.2</b> Variance of a categorical variable?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling-variation.html"><a href="modeling-variation.html"><i class="fa fa-check"></i><b>4</b> Modeling variation</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling-variation.html"><a href="modeling-variation.html#statistical-models"><i class="fa fa-check"></i><b>4.1</b> Statistical models</a></li>
<li class="chapter" data-level="4.2" data-path="modeling-variation.html"><a href="modeling-variation.html#quantitative-response-variables"><i class="fa fa-check"></i><b>4.2</b> Quantitative response variables</a></li>
<li class="chapter" data-level="4.3" data-path="modeling-variation.html"><a href="modeling-variation.html#proportions-and-indicator-variables"><i class="fa fa-check"></i><b>4.3</b> Proportions and indicator variables</a></li>
<li class="chapter" data-level="4.4" data-path="modeling-variation.html"><a href="modeling-variation.html#a-taxonomy-of-simple-models"><i class="fa fa-check"></i><b>4.4</b> A taxonomy of simple models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-values.html"><a href="model-values.html"><i class="fa fa-check"></i><b>5</b> Model values</a><ul>
<li class="chapter" data-level="5.1" data-path="model-values.html"><a href="model-values.html#model-fitting-a-contest-between-candidate-models"><i class="fa fa-check"></i><b>5.1</b> Model fitting: A contest between candidate models</a></li>
<li class="chapter" data-level="5.2" data-path="model-values.html"><a href="model-values.html#variance-of-model-values"><i class="fa fa-check"></i><b>5.2</b> Variance of model values</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html"><i class="fa fa-check"></i><b>6</b> Degrees of flexibility</a><ul>
<li class="chapter" data-level="6.1" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#one-degree-of-flexibility"><i class="fa fa-check"></i><b>6.1</b> One degree of flexibility</a></li>
<li class="chapter" data-level="6.2" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#multiple-degrees-of-flexibility"><i class="fa fa-check"></i><b>6.2</b> Multiple degrees of flexibility</a></li>
<li class="chapter" data-level="6.3" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#covariates"><i class="fa fa-check"></i><b>6.3</b> Covariates</a></li>
<li class="chapter" data-level="6.4" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#flexibility-literally"><i class="fa fa-check"></i><b>6.4</b> Flexibility, literally</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="effect-size.html"><a href="effect-size.html"><i class="fa fa-check"></i><b>7</b> Effect size</a><ul>
<li class="chapter" data-level="7.1" data-path="effect-size.html"><a href="effect-size.html#slopes-and-differences"><i class="fa fa-check"></i><b>7.1</b> Slopes and differences</a></li>
<li class="chapter" data-level="7.2" data-path="effect-size.html"><a href="effect-size.html#risk"><i class="fa fa-check"></i><b>7.2</b> Risk</a></li>
<li class="chapter" data-level="7.3" data-path="effect-size.html"><a href="effect-size.html#simple-changes-in-input"><i class="fa fa-check"></i><b>7.3</b> Simple changes in input</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-and-r.html"><a href="f-and-r.html"><i class="fa fa-check"></i><b>8</b> F and R</a><ul>
<li class="chapter" data-level="8.1" data-path="f-and-r.html"><a href="f-and-r.html#whats-the-meaning-of-f"><i class="fa fa-check"></i><b>8.1</b> What’s the meaning of F?</a></li>
<li class="chapter" data-level="8.2" data-path="f-and-r.html"><a href="f-and-r.html#r-squared"><i class="fa fa-check"></i><b>8.2</b> R-squared</a></li>
<li class="chapter" data-level="8.3" data-path="f-and-r.html"><a href="f-and-r.html#f-in-statistics-books"><i class="fa fa-check"></i><b>8.3</b> F in statistics books</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Confidence intervals</a><ul>
<li class="chapter" data-level="9.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-from-f"><i class="fa fa-check"></i><b>9.1</b> Confidence intervals from F</a></li>
<li class="chapter" data-level="9.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#why-4"><i class="fa fa-check"></i><b>9.2</b> Why 4?</a></li>
<li class="chapter" data-level="9.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#situations-where-f-doesnt-tell-enough"><i class="fa fa-check"></i><b>9.3</b> Situations where F doesn’t tell enough</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html"><i class="fa fa-check"></i><b>10</b> So-called “statistical significance”</a><ul>
<li class="chapter" data-level="10.1" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#calculating-a-p-value"><i class="fa fa-check"></i><b>10.1</b> Calculating a p-value</a></li>
<li class="chapter" data-level="10.2" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#history-and-criticism"><i class="fa fa-check"></i><b>10.2</b> History and criticism</a></li>
<li class="chapter" data-level="10.3" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#appendix-when-df-geq-2"><i class="fa fa-check"></i><b>10.3</b> Appendix: When <span class="math inline">\(df \geq 2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html"><i class="fa fa-check"></i><b>11</b> Simple means and proportions</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html#no-flexibility-df-0"><i class="fa fa-check"></i><b>11.1</b> No flexibility: df = 0</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="comparing-models.html"><a href="comparing-models.html"><i class="fa fa-check"></i><b>12</b> Comparing models</a></li>
<li class="chapter" data-level="13" data-path="outside-of-the-normal.html"><a href="outside-of-the-normal.html"><i class="fa fa-check"></i><b>13</b> Outside of the normal</a></li>
<li class="chapter" data-level="14" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html"><i class="fa fa-check"></i><b>14</b> Remember, inference isn’t everything</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/Compact_inference" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Compact Guide to Classical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="measuring-variation" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Measuring variation</h1>
<p>Recall that the purpose of statistical inference is to determine which statistical claims are justified by the data on which they are based. This amounts to asking whether the data provide enough evidence to support a claim. How can we figure out how much data is enough?</p>
<p>An obvious, and important way to quantify “how much” is the number of rows in the data frame, that is, the <em>sample size</em> <span class="math inline">\(n\)</span>. Perhaps it’s intuitive that more data constitutes more evidence. Some care is required here, since we want to avoid phony creation of large sample sizes by copying earlier rows to make new rows in the data frame. One proper procedure is to insist that each unit of analysis be grabbed at random from a <em>population</em> of all the possible units. A data frame constructed by such a procedure is called a <em>sample of the population</em>, which is why the number of rows <span class="math inline">\(n\)</span> is called the sample size.</p>
<p>It’s tempting to elaborate on <em>how much</em> evidence we have by counting the number of variables in the data frame. But there is a serious problem here. But there is no such thing as the “set of possible variables.” It’s the researcher who determines what will be a variable, and you can in principle make up as many as you like. In the running example from Chapter 1, the variables were age and running time. Sensible. But we might also have recorded the runner’s favorite number, or the time the runner’s brother had breakfast the Tuesday before the race, or anything else, relevant or not. Common sense tells you to avoid such silliness. But what one person considers silly might be sensible to someone else. For instance, many people take seriously astrological signs, but others don’t. Should we count astrological sign as a genuine variable? As it happens, birth month accounts for some of the observed differences in performance of professional athletes. (The reason appears to be that children who are the oldest in their school grade do better as kids in athletics, which leads to them developing confidence and interest in sports and receiving extra attention from coaches.)</p>
<p>The key to measuring <em>how much</em> evidence the data provides lies in the sentence, “Birth month accounts for some of the observed differences in performance.” What matters is whether a variable can <em>explain</em> or <em>account</em> for the variation in a outcome of interest (like athletic performance). We need to be able to say how much variation is in the outcome. As described in the previous chapter, in a statistical model the outcome is represented by the response variable. We’ll measure the variation in the response variable and then compare it to the amount of variation that the statistical model attributes to the explanatory variable(s).</p>
<div id="variance-of-a-numerical-variable" class="section level2">
<h2><span class="header-section-number">3.1</span> Variance of a numerical variable</h2>
<p>Recall that the statistical models we use in this book will always have a numerical response variable. We can quantify the amount of variation the response variable in many different ways. The conventional way is by a quantity called the <em>variance</em>.</p>
<p>There are different ways to calculate the variance of a variable. Most textbooks give a formula that can be used efficiently by a computer. For the purpose of explaining the variance to another person, I like another way.</p>
<p>The starting point is the response variable for which you want to know the variance. Usually, we organize variables into data frames, but for the moment imagine that the individual numbers, <span class="math inline">\(n\)</span> of them, have been spilled out on the surface of a table. Take two of the numbers at random. Chances are, the two numbers are different but they might, by luck, be exactly the same. Doesn’t matter. To measure the variation of these two numbers, simply subtract one from the other to get the difference, then square the difference. Because of the squaring, it doesn’t matter whether you subtract the first number from the second or <em>vice versa</em>. For historical reasons, the <em>variance</em> is the square difference divided by two. But if history had worked out differently, the square difference would have been a fine measure of variation itself.</p>
<p>The square difference measures the variation between two numbers. But we want to measure the variation of the whole set of numbers. To do this, repeat the calculation of the square difference for <em>every possible pair of the numbers on the table</em>. For instance, if there were <span class="math inline">\(n=3\)</span> numbers, say</p>
<p><span class="math display">\[5, 9,  3\]</span>
the pairs would be</p>
<ul>
<li>5 - 9 giving a difference of -4 which squares to 16</li>
<li>5 - 3 giving a difference of 2, which squares to 4</li>
<li>3 - 9 giving a difference of -6, which squares to 36</li>
</ul>
<p>Now average all the square differences. Averaging 16, 4, 36 gives 18.67. The variance, by historical convention, is half this number, or 9.33.</p>
<p>When <span class="math inline">\(n\)</span> is big, there are a lot of possible pairs of numbers. For instance, when <span class="math inline">\(n = 100\)</span>, there are 4950 pairs. That’s why we leave it to the computer to do the calculation, and even then the calculation is re-arranged so that there are only 100 square differences involved.</p>
<p>If you like, you can think of the reason why we square the difference as a convenience to avoid having to worry about whether the difference is positive or negative (which depends only on which of the pair of values you put first in the subtraction). But there is some profound thinking behind the use of squares, which reflects the nature of randomness and, believe it or not, the Pythagorean theorem.</p>
</div>
<div id="variance-of-a-categorical-variable" class="section level2">
<h2><span class="header-section-number">3.2</span> Variance of a categorical variable?</h2>
<p>A categorical variable has distinct <em>levels</em>, usually represented by labels such as <em>agree</em>, <em>undecided</em>, and <em>disagree</em>. To attempt to describe the amount of variation in a categorical variable we can follow the same process as for numerical variables: spill the collection of <span class="math inline">\(n\)</span> labels onto a table, pick at random a pair of labels, subtract them, and square the difference.</p>
<p>There’s a big problem, however. What is the numerical value of the difference between <em>agree</em> and <em>undecided</em>? How does the size of the difference between <em>agree</em> and <em>undecided</em> compare to the difference between <em>disagree</em> and <em>undecided</em> or between <em>agree</em> and <em>disagree</em>? Sometimes there’s a reasonable choice to be made, for example we might decide that <em>agree</em> and <em>disaggree</em> differ by 2, <em>agree</em> and <em>undecided</em> differ by 1, and that <em>disagree</em> and <em>undecided</em> also differ by 1. Even more basic, it’s reasonable to say that the difference between <em>agree</em> and <em>agree</em> should be zero, and similarly for <em>disagree</em> versus <em>disagree</em> or <em>undecided</em> versus <em>undecided</em>.</p>
<p>Notice that all these declared differences can be created by recoding the categorical variable as a numeric variable. For instance, we can change <em>agree</em> to 1, <em>undecided</em> to 2, and <em>disagree</em> to 3. Then just calculate the variance of the numerical variable in the usual way.</p>
<p>Sometimes it’s sensible to translate the levels of a categorical variable into numbers. For instance, with <em>agree</em>/<em>undecided</em>/<em>disagree</em> it’s reasonable to think that <em>undecided</em> is inbetween <em>agree</em> and <em>disagree</em>. But, in general, there will be no such sense of inbetweenness of categorical levels. Take, for example, a categorical variable whose levels are the names of countries. Or a categorical variable whose levels are political parties: Green, Libertarian, Democratic, Republican. Which levels are between which? (As it happens, people do try to put political parties in sequential order by categorizing them on the scale from Left to Right.)</p>
<p>Without a sense of <em>inbetweenness</em> of levels, it’s arbitrary to assign numbers to the various levels. Except in one situation.</p>
<p>Often, categorical variables have only two levels. Yes or no. Dead or alive. Accepted or rejected. Treatment and control. Such variables are sometimes called <em>binary</em> (like the 0/1 of computer bits) or <em>dicotomous</em> or <em>binomial</em> (meaning, having two names) or even <em>two-level</em>. In the previous chapter, we called them <em>indicator</em> variables.</p>
<p>When dealing with an indicator variable, there’s no level to be inbetween; there are only two levels and the idea of “in between” requires at least three distinct things. So we can easily agree, regardless of our opinions about how the world works, that the difference is zero between labels that are the same (say, <em>yes</em> and <em>yes</em> or between <em>no</em> and <em>no</em>). And when the labels are different (say, <em>yes</em> and <em>no</em>) we just need to assign a non-zero number to the difference.</p>
<p>Which number? Should the square-difference between <em>yes</em> and <em>no</em> be 17, or 328, or 0.3? By convention, we use the number 1 for the square-difference between the two levels of a binary variable. This convention has the advantage of simplifying calculations. It’s also what you will get by treating indicator variables numerically. But there is another important advantage of the simple choice: any average of a 0/1 variable must always be somewhere in the range from 0 to 1, which is exactly the same scale we use for describing <em>probability</em>.</p>
<p>The simplicity of dealing with indicator variables means that the techniques of statistical inference with an indicator for a categorical response variable are much easier than for non-binary categorical response variables. This is also the most common setting for classical inference.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-and-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling-variation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Compact_inference.pdf", "Compact_inference.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

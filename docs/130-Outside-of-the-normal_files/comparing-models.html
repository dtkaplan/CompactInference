<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Comparing models | A Compact Guide to Classical Inference</title>
  <meta name="description" content="Chapter 12 Comparing models | A Compact Guide to Classical Inference" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Comparing models | A Compact Guide to Classical Inference" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Comparing models | A Compact Guide to Classical Inference" />
  
  
  

<meta name="author" content="Daniel Kaplan" />


<meta name="date" content="2020-03-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-means-and-proportions.html"/>
<link rel="next" href="remember-inference-isnt-everything.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Compact Guide to Classical Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html"><i class="fa fa-check"></i><b>1</b> What is classical inference?</a><ul>
<li class="chapter" data-level="" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html#and-why-should-i-read-this-book"><i class="fa fa-check"></i>… and why should I read this book?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-and-variables.html"><a href="data-and-variables.html"><i class="fa fa-check"></i><b>2</b> Data and variables</a><ul>
<li class="chapter" data-level="2.1" data-path="data-and-variables.html"><a href="data-and-variables.html#data-frames"><i class="fa fa-check"></i><b>2.1</b> Data frames</a></li>
<li class="chapter" data-level="2.2" data-path="data-and-variables.html"><a href="data-and-variables.html#tabulations"><i class="fa fa-check"></i><b>2.2</b> Tabulations</a></li>
<li class="chapter" data-level="2.3" data-path="data-and-variables.html"><a href="data-and-variables.html#quantitative-and-categorical-variables"><i class="fa fa-check"></i><b>2.3</b> Quantitative and categorical variables</a></li>
<li class="chapter" data-level="2.4" data-path="data-and-variables.html"><a href="data-and-variables.html#response-and-explanatory-variables"><i class="fa fa-check"></i><b>2.4</b> Response and explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="measuring-variation.html"><a href="measuring-variation.html"><i class="fa fa-check"></i><b>3</b> Measuring variation</a><ul>
<li class="chapter" data-level="3.1" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-numerical-variable"><i class="fa fa-check"></i><b>3.1</b> Variance of a numerical variable</a></li>
<li class="chapter" data-level="3.2" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-categorical-variable"><i class="fa fa-check"></i><b>3.2</b> Variance of a categorical variable?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling-variation.html"><a href="modeling-variation.html"><i class="fa fa-check"></i><b>4</b> Modeling variation</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling-variation.html"><a href="modeling-variation.html#statistical-models"><i class="fa fa-check"></i><b>4.1</b> Statistical models</a></li>
<li class="chapter" data-level="4.2" data-path="modeling-variation.html"><a href="modeling-variation.html#quantitative-response-variables"><i class="fa fa-check"></i><b>4.2</b> Quantitative response variables</a></li>
<li class="chapter" data-level="4.3" data-path="modeling-variation.html"><a href="modeling-variation.html#proportions-and-indicator-variables"><i class="fa fa-check"></i><b>4.3</b> Proportions and indicator variables</a></li>
<li class="chapter" data-level="4.4" data-path="modeling-variation.html"><a href="modeling-variation.html#taxonomy"><i class="fa fa-check"></i><b>4.4</b> A taxonomy of simple models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-values.html"><a href="model-values.html"><i class="fa fa-check"></i><b>5</b> Model values</a><ul>
<li class="chapter" data-level="5.1" data-path="model-values.html"><a href="model-values.html#model-fitting-a-contest-between-candidate-models"><i class="fa fa-check"></i><b>5.1</b> Model fitting: A contest between candidate models</a></li>
<li class="chapter" data-level="5.2" data-path="model-values.html"><a href="model-values.html#variance-of-model-values"><i class="fa fa-check"></i><b>5.2</b> Variance of model values</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html"><i class="fa fa-check"></i><b>6</b> Degrees of flexibility</a><ul>
<li class="chapter" data-level="6.1" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#one-degree-of-flexibility"><i class="fa fa-check"></i><b>6.1</b> One degree of flexibility</a></li>
<li class="chapter" data-level="6.2" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#multiple-degrees-of-flexibility"><i class="fa fa-check"></i><b>6.2</b> Multiple degrees of flexibility</a></li>
<li class="chapter" data-level="6.3" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#covariates"><i class="fa fa-check"></i><b>6.3</b> Covariates</a></li>
<li class="chapter" data-level="6.4" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#flexibility-literally"><i class="fa fa-check"></i><b>6.4</b> Flexibility, literally</a></li>
<li class="chapter" data-level="6.5" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#degrees-of-flexibility-and-freedom"><i class="fa fa-check"></i><b>6.5</b> Degrees of flexibility and freedom</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="effect-size.html"><a href="effect-size.html"><i class="fa fa-check"></i><b>7</b> Effect size</a><ul>
<li class="chapter" data-level="7.1" data-path="effect-size.html"><a href="effect-size.html#with-respect-to"><i class="fa fa-check"></i><b>7.1</b> With respect to …</a></li>
<li class="chapter" data-level="7.2" data-path="effect-size.html"><a href="effect-size.html#slopes-and-differences"><i class="fa fa-check"></i><b>7.2</b> Slopes and differences</a></li>
<li class="chapter" data-level="7.3" data-path="effect-size.html"><a href="effect-size.html#risk"><i class="fa fa-check"></i><b>7.3</b> Risk</a></li>
<li class="chapter" data-level="7.4" data-path="effect-size.html"><a href="effect-size.html#simple-changes-in-input"><i class="fa fa-check"></i><b>7.4</b> Simple changes in input</a></li>
<li class="chapter" data-level="7.5" data-path="effect-size.html"><a href="effect-size.html#reading-effect-size-from-a-graph"><i class="fa fa-check"></i><b>7.5</b> Reading effect size from a graph</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-and-r.html"><a href="f-and-r.html"><i class="fa fa-check"></i><b>8</b> F and R</a><ul>
<li class="chapter" data-level="8.1" data-path="f-and-r.html"><a href="f-and-r.html#the-f-statistic"><i class="fa fa-check"></i><b>8.1</b> The F statistic</a></li>
<li class="chapter" data-level="8.2" data-path="f-and-r.html"><a href="f-and-r.html#whats-the-meaning-of-f"><i class="fa fa-check"></i><b>8.2</b> What’s the meaning of F?</a></li>
<li class="chapter" data-level="8.3" data-path="f-and-r.html"><a href="f-and-r.html#r-squared"><i class="fa fa-check"></i><b>8.3</b> R-squared</a></li>
<li class="chapter" data-level="8.4" data-path="f-and-r.html"><a href="f-and-r.html#f-in-statistics-books"><i class="fa fa-check"></i><b>8.4</b> F in statistics books</a></li>
<li class="chapter" data-level="8.5" data-path="f-and-r.html"><a href="f-and-r.html#another-explanation-of-f"><i class="fa fa-check"></i><b>8.5</b> Another explanation of F</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Confidence intervals</a><ul>
<li class="chapter" data-level="9.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-confidence-intervals-on-effect-size"><i class="fa fa-check"></i><b>9.1</b> Calculating confidence intervals on effect size</a></li>
<li class="chapter" data-level="9.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#and-95"><i class="fa fa-check"></i><b>9.2</b> 4 and 95%</a></li>
<li class="chapter" data-level="9.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#and-n"><i class="fa fa-check"></i><b>9.3</b> 4 and <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="9.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-versus-prediction-intervals"><i class="fa fa-check"></i><b>9.4</b> Confidence versus prediction intervals</a></li>
<li class="chapter" data-level="9.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#for-the-conventionally-trained-reader"><i class="fa fa-check"></i><b>9.5</b> For the conventionally trained reader …</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html"><i class="fa fa-check"></i><b>10</b> So-called “statistical significance”</a><ul>
<li class="chapter" data-level="10.1" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#calculating-a-p-value"><i class="fa fa-check"></i><b>10.1</b> Calculating a p-value</a></li>
<li class="chapter" data-level="10.2" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#history-and-criticism"><i class="fa fa-check"></i><b>10.2</b> History and criticism</a></li>
<li class="chapter" data-level="10.3" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#appendix-when-df-geq-2"><i class="fa fa-check"></i><b>10.3</b> Appendix: When <span class="math inline">\(df \geq 2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html"><i class="fa fa-check"></i><b>11</b> Simple means and proportions</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html#the-standard-error-of-the-mean-and-the-proportion"><i class="fa fa-check"></i><b>11.1</b> The standard error of the mean and the proportion</a></li>
<li class="chapter" data-level="11.2" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html#equivalencies-with-b-f-and-v_m"><i class="fa fa-check"></i><b>11.2</b> Equivalencies with B, F, and v_m</a></li>
<li class="chapter" data-level="11.3" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html#t-and-z"><i class="fa fa-check"></i><b>11.3</b> t and z</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="comparing-models.html"><a href="comparing-models.html"><i class="fa fa-check"></i><b>12</b> Comparing models</a><ul>
<li class="chapter" data-level="12.1" data-path="comparing-models.html"><a href="comparing-models.html#complexity-and-cost"><i class="fa fa-check"></i><b>12.1</b> Complexity and cost</a></li>
<li class="chapter" data-level="12.2" data-path="comparing-models.html"><a href="comparing-models.html#why-f"><i class="fa fa-check"></i><b>12.2</b> Why F?</a></li>
<li class="chapter" data-level="12.3" data-path="comparing-models.html"><a href="comparing-models.html#analysis-of-variance"><i class="fa fa-check"></i><b>12.3</b> Analysis of variance</a></li>
<li class="chapter" data-level="12.4" data-path="comparing-models.html"><a href="comparing-models.html#analysis-of-co-variance"><i class="fa fa-check"></i><b>12.4</b> Analysis of co-variance</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html"><i class="fa fa-check"></i><b>13</b> Remember, inference isn’t everything</a><ul>
<li class="chapter" data-level="13.1" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html#substantiality"><i class="fa fa-check"></i><b>13.1</b> Substantiality</a></li>
<li class="chapter" data-level="13.2" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html#causal-reasoning"><i class="fa fa-check"></i><b>13.2</b> Causal reasoning</a></li>
<li class="chapter" data-level="13.3" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html#confounding"><i class="fa fa-check"></i><b>13.3</b> Confounding</a></li>
<li class="chapter" data-level="13.4" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html#the-stars-of-statistical-inference"><i class="fa fa-check"></i><b>13.4</b> The stars of statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="getting-started-in-your-classroom.html"><a href="getting-started-in-your-classroom.html"><i class="fa fa-check"></i><b>14</b> Getting started in your classroom</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/Compact_inference" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Compact Guide to Classical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="comparing-models" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Comparing models</h1>
<p>Up to now, we have looked at models individually. It’s time now to examine models in <em>pairs</em>. Such a perspective gives insight into <em>constructing</em> models, for example by including a new explanatory variable in addition to the old ones, or increasing the curvature used in a model function.</p>
<p>The basic strategy is simple. Start with an existing model, which we might call <span class="math inline">\(\hbox{mod}_1\)</span>. Then extend or elaborate <span class="math inline">\(\hbox{mod}_1\)</span> in some way, perhaps by including a new explanatory variable to produce <span class="math inline">\(\hbox{mod}_2\)</span>. We can then examine the <em>increase</em> in the variance of the model values from what it was under <span class="math inline">\(\hbox{mod}_1\)</span> to what it will be under <span class="math inline">\(\hbox{mod}_2\)</span>.</p>
<p>Since we are working with two models, it’s helpful to modify the notation a bit. Before, we had <span class="math inline">\(\hbox{v}_r\)</span> and <span class="math inline">\(\hbox{v}_m\)</span>, the variance of the response variable and of the model values respectively. We also had R<sup>2</sup>, which is simply <span class="math inline">\(\hbox{v}_m\)</span> divided by <span class="math inline">\(\hbox{v}_r\)</span>. We’ll keep <span class="math inline">\(\hbox{v}_r\)</span> as standing for the variance of the response variable but write <span class="math inline">\(\hbox{v}_1\)</span> and <span class="math inline">\(\hbox{v}_2\)</span> as the variance of the model values from <span class="math inline">\(\hbox{mod}_1\)</span> and <span class="math inline">\(\hbox{mod}_2\)</span> respectively.</p>
<p>Likewise, it’s useful to construct the R-squared ratios for the two models, which we’ll denote <span class="math inline">\(\hbox{R}^2_1\)</span> and <span class="math inline">\(\hbox{R}^2_2\)</span>, and which are simply <span class="math inline">\(\hbox{v}_1 / \hbox{v}_r\)</span> and <span class="math inline">\(\hbox{v}_2 / \hbox{v}_r\)</span> respectively.</p>
<div id="complexity-and-cost" class="section level2">
<h2><span class="header-section-number">12.1</span> Complexity and cost</h2>
<p>A central question when considering an extending <span class="math inline">\(\hbox{mod}_1\)</span> into <span class="math inline">\(\hbox{mod}_2\)</span> is whether the resulting increase in variances, <span class="math inline">\(\Delta \mbox{v} = \hbox{v}_2 - \hbox{v}_1\)</span> is large enough to justify concluding that extending <span class="math inline">\(\hbox{mod}_1\)</span> to <span class="math inline">\(\hbox{mod}_2\)</span> is worth the cost.</p>
<p>The “cost?” What cost? Computing the new model <span class="math inline">\(\hbox{mod}_2\)</span> and its model values is, with a computer, trivial and costs nothing. Rather, the cost of concern is the added <em>complexity</em> of <span class="math inline">\(\hbox{mod}_2\)</span> compared to <span class="math inline">\(\hbox{mod}_1\)</span>.</p>
<p>There is a tradition in science of assuming that, all other things being equal, simpler models are better than more complicated models. Sometimes this tradition is expressed as the “law of parsimony,” as if it were a physical principle (which it is not, until you get very deep into things as in quantum electrodynamics). Sometimes it is given the name “Occams’s Razor” in honor of William of Ockham (c. 1287–1347). Occam’s Latin statement is</p>
<blockquote>
<p>“<em>Numquam ponenda est pluralitas sine necessitate.</em>”</p>
</blockquote>
<p>The English translation is scarcely better: “Plurality must never be posited without necessity.” A somewhat earlier formulation might be clearer:</p>
<blockquote>
<p><em>“It is superfluous to suppose that what can be accounted for by a few principles has been produced by many.”</em> – Thomas Aquinas (1225–1274)</p>
</blockquote>
<p>It’s deeply ironic that opinions from the 13th century be quoted as support for scientific principles. At the start of the 20th century, however, some relevant mathematics started to emerge, the mathematics of random walks. This new mathematics provided insight into how complexity of a model might be measured and a criterion for judging whether adding complexity is worthwhile.</p>
<p>We’ll put the mathematical theory in a framework that we’ve already developed in earlier chapters. The ingredients are</p>
<ul>
<li>n, the number of rows in the data used to construct the model</li>
<li><span class="math inline">\(^\circ\!{\cal{F}}_1\)</span> and <span class="math inline">\(^\circ\!{\cal{F}}_0\)</span> the degrees of flexibility of <span class="math inline">\(\hbox{mod}_2\)</span> and <span class="math inline">\(\hbox{mod}_1\)</span> respectively.</li>
<li><span class="math inline">\(\hbox{R}^2_2\)</span> and <span class="math inline">\(\hbox{R}^2_1\)</span></li>
</ul>
<p>Here are some relevant facts resulting from the mathematical theory of random walks.</p>
<ul>
<li><p>Fact 1: If <span class="math inline">\(\hbox{mod}_2\)</span> is an <em>extension</em> of <span class="math inline">\(\hbox{mod}_1\)</span> (for instance, one created by including an additional explanatory variable) then <span class="math inline">\(\hbox{v}_2 \geq \hbox{v}_1\)</span>.</p>
<p>Justification: Reach back to Chapter 5 where we considered the process of creating a model function. Recall that the “best” model function is one that comes as close to the response variable as possible, given the selected set of explanatory variables. In extending <span class="math inline">\(\hbox{mod}_1\)</span>, we are giving the process an additional variable to work with. Likely, there will be some way to use that additional variable so that <span class="math inline">\(\hbox{mod}_2\)</span> gets even closer to the response variable than <span class="math inline">\(\hbox{mod}_1\)</span>. But simply by <em>ignoring</em> the additional variable, <span class="math inline">\(\hbox{mod}_2\)</span> can at least as close as <span class="math inline">\(\hbox{mod}_2\)</span>.</p></li>
<li><p>Fact 2: Suppose that all of the explanatory variables are unrelated to the response variable, for instance, they have been created at random. Then a model constructed with n-1 degrees of flexibility will have <span class="math inline">\(\hbox{v}_m = \hbox{v}_r\)</span>. What’s more, a model with <span class="math inline">\(p &lt; n-1\)</span> degrees of freedom will, on average, have <span class="math inline">\(\hbox{v}_m \approx \frac{p}{n-1} \hbox{v}_r\)</span>. Another way of stating this is that
<span class="math display">\[R^2 = \frac{\hbox{v}_m}{\hbox{v}_r} \approx \frac{p}{n-1}\]</span></p>
<p>In support of Fact 2, we’ll do some simulations. In each trial, we’ll generate a small data set with <span class="math inline">\(n=11\)</span>. The response variable will come from a random number generator. Similarly we’ll make up p=5 explanatory variables, each of which is also from a random number generator. Then we’ll calculate R<sup>2</sup> from each model and look at the distribution. According to the principle, on average</p>
<p><span class="math inline">\(R^2 = \frac{p}{n-1} = \frac{5}{10}\)</span>.</p></li>
</ul>
<p>We’ll do the simulation using R software and the <code>mosaic</code> package. I provide the code so that anyone who wants to carry out their own simulation can do so. We’ll do 1000 trials.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="comparing-models.html#cb1-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb1-2"><a href="comparing-models.html#cb1-2"></a></span>
<span id="cb1-3"><a href="comparing-models.html#cb1-3"></a>Trials &lt;-<span class="st">  </span><span class="kw">do</span>(<span class="dv">1000</span>) <span class="op">*</span><span class="st"> </span>(</span>
<span id="cb1-4"><a href="comparing-models.html#cb1-4"></a>  <span class="kw">data.frame</span>(<span class="dt">y  =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">11</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1-5"><a href="comparing-models.html#cb1-5"></a><span class="st">    </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">rand</span>(<span class="dv">5</span>), <span class="dt">data =</span> .) <span class="op">%&gt;%</span></span>
<span id="cb1-6"><a href="comparing-models.html#cb1-6"></a><span class="st">    </span><span class="kw">rsquared</span>()</span>
<span id="cb1-7"><a href="comparing-models.html#cb1-7"></a>)</span>
<span id="cb1-8"><a href="comparing-models.html#cb1-8"></a><span class="kw">mean</span>( <span class="op">~</span><span class="st"> </span>result,  <span class="dt">data =</span> Trials)</span></code></pre></div>
<pre><code>## [1] 0.498377</code></pre>
<p>You can repeat the simulation with any n and p that you like to confirm that, on average, <span class="math inline">\(R^2 \approx \frac{p}{n-1}\)</span>.</p>
<pre><code>## Warning: `data_frame()` is deprecated, use `tibble()`.
## This warning is displayed once per session.</code></pre>
</div>
<div id="why-f" class="section level2">
<h2><span class="header-section-number">12.2</span> Why F?</h2>
<p>We can use Fact 2 to explain why it makes sense for the F-statistic to be defined the way it is. Consider a graph of the average R<sup>2</sup> versus <span class="math inline">\(^\circ\!{\cal{F}}\)</span> when all the explanatory variables are made with a random number generator</p>

<div class="figure"><span id="fig:randomR"></span>
<img src="120-Comparing-models_files/figure-html/randomR-1.png" alt="(ref:RandomR-cap)" width="672" />
<p class="caption">
Figure 12.1: (ref:RandomR-cap)
</p>
</div>
<p>In Figure 12.1, the blue point falls at R<sup>2</sup>=0.65 and <span class="math inline">\(^\circ\!{\cal{F}} = 3\)</span>. Referring to the individual random trials with <span class="math inline">\(^\circ\!{\cal{F}} = 3\)</span> (the black dots), you can see that a very small percentage have R<sup>2</sup> &gt; 0.65. This alone would cause one to doubt that the explanatory variables used in making the blue dot were purely random. In fact, a p-value for the blue dot could be calculated simply by counting the proportion of black dots that have R-squared larger than the blue dot (for <span class="math inline">\(^\circ\!{\cal{F}} = 3\)</span>).</p>
<p>This book is about <em>classical inference</em>, so the use of simulation, a computer-era technique, is out of place. Classical inference resorts to a different observation. In Figure 12.1, there are two blue segments. One connects the origin to the blue dot, the other connects the blue dot to the upper-right corner of the plot. We’ll call the left-most segment the “model segment,” since it reflects Clearly the slope of segment one is larger than the slope of segment two. This will happen whenever the blue dot is above the vertical range of the random trials. But for the random trials, on average, the two slopes would be about the same. So, under the Null Hypothesis that the explanatory variables are unrelated to the response variable, the ratio of slopes will be about 1.</p>
<p>Fisher suggested using the ratio of slopes in the R<sup>2</sup> versus <span class="math inline">\(^\circ\!{\cal{F}} = 3\)</span> graph as a test statistic. Given a blue dot with its R<sup>2</sup> and <span class="math inline">\(^\circ\!{\cal{F}} = 3\)</span>, the slope of segment one is <span class="math inline">\(\mbox{R}^2_1 / ^\circ\!{\cal{F}}\)</span>. Similarly, the slope of segment two is
<span class="math inline">\((1 - \mbox{R}^2_2) / (n - (^\circ\!{\cal{F}} +1))\)</span>. The ratio of slope one to slope two is F.</p>

<div class="figure"><span id="fig:F-vs-df"></span>
<img src="120-Comparing-models_files/figure-html/F-vs-df-1.png" alt="Figure 12.2. Translating the \(R^2\) and \(^\circ\!{\cal{F}}\) values from Figure 12.1 into the corresponding F value shows a much simpler version. On average, the F values from the random simulation are about \(F=1\) regardless of \(^\circ\!{\cal{F}}\). Similarly, almost all the random trials produce \(F &lt; 4\). Fisher deduced these facts using algebra and calculus with the mathematics of random walks, a remarkable achievement. Today, it takes only a moderate amount of computer-programming skill to confirm them." width="672" />
<p class="caption">
Figure 12.2: Figure 12.2. Translating the <span class="math inline">\(R^2\)</span> and <span class="math inline">\(^\circ\!{\cal{F}}\)</span> values from Figure 12.1 into the corresponding F value shows a much simpler version. On average, the F values from the random simulation are about <span class="math inline">\(F=1\)</span> regardless of <span class="math inline">\(^\circ\!{\cal{F}}\)</span>. Similarly, almost all the random trials produce <span class="math inline">\(F &lt; 4\)</span>. Fisher deduced these facts using algebra and calculus with the mathematics of random walks, a remarkable achievement. Today, it takes only a moderate amount of computer-programming skill to confirm them.
</p>
</div>
</div>
<div id="analysis-of-variance" class="section level2">
<h2><span class="header-section-number">12.3</span> Analysis of variance</h2>
<p>The word “analysis” refers to the process of breaking something down into its constituent components. (The antonym is “synthesis,” the process of bringinging components together to form a whole.)</p>
<p>The phrase “analysis of variance” concisely describes the strategy for examining a model that consists of components. For example, we’ve described <span class="math inline">\(\hbox{mod}_2\)</span> as being an extension of <span class="math inline">\(\hbox{mod}_1\)</span> – mod_2 consists of <span class="math inline">\(\hbox{mod}_1\)</span> plus some additional component.</p>
<p>In undertaking analysis of variance, the quantity that is being broken down is the variance <em>r</em> of the response variable. When working with a single model, the variance is broken down into two components:</p>
<ul>
<li><em>m</em>, the variance accounted for by the model</li>
<li>what’s left over, the residual variance, which is simply <em>r</em> - <em>m</em>.</li>
</ul>
<p>In the previous section, I’ve chosen to report not the variance itself, but the <em>proportion</em> of the variance of the response variable:</p>
<ul>
<li><span class="math inline">\(R^2 = \hbox{v}_m / \hbox{v}_r\)</span>, the proportion accounted for by the model</li>
<li><span class="math inline">\(1 - R^2 = (\hbox{v}_r - \hbox{v}_m) / \hbox{v}_r\)</span>, the residual proportion of the variance.</li>
</ul>
<p>The idea of analysis of variance can be extended to the situation of a model <span class="math inline">\(\hbox{mod}_2\)</span> which has been built as an extension to a model <span class="math inline">\(\hbox{mod}_1\)</span>. The situation is sketched in Figure 12.3. First, calculate <span class="math inline">\(\hbox{R}^2_1\)</span>, the proportion of the variance accounted for by <span class="math inline">\(\hbox{mod}_1\)</span>. Second, calculate <span class="math inline">\(\hbox{R}^2_2\)</span>, the proportion of the variance captured by <span class="math inline">\(\hbox{mod}_2\)</span>. In Figure 12.3, <span class="math inline">\(\hbox{R}^2_1\)</span> is the left-most blue dot, while <span class="math inline">\(\hbox{R}^2_2\)</span> is the other blue dot.</p>
<p>The two blue dots divide the domain of the graph into three components: the model-one segment, the model-two segment, and the residual segment.</p>
<p>To address the question of whether <span class="math inline">\(\hbox{mod}_2\)</span> meaningfully extends <span class="math inline">\(\hbox{mod}_1\)</span>, we compare the slope of the model-two segment to that of the residual segment. This ratio of slopes, which we can call <span class="math inline">\(\Delta \hbox{F}\)</span>, is the test statistic. If <span class="math inline">\(Delta\hbox{F} \gtrapprox 4\)</span>, it’s fair to conclude that the extension to <span class="math inline">\(\hbox{mod}_1\)</span> is discernably different from an extension that would have been concocted from random explanatory variables unrelated to the response variable.</p>

<div class="figure"><span id="fig:two-models"></span>
<img src="120-Comparing-models_files/figure-html/two-models-1.png" alt="Figure 12.3: looking at how the extensions to \(\hbox{mod}_1\) contained in \(\hbox{mod}_2\) increase \(\mbox{R}^2_2\) compared to \(\mbox{R}^2_1\)" width="672" />
<p class="caption">
Figure 12.3: Figure 12.3: looking at how the extensions to <span class="math inline">\(\hbox{mod}_1\)</span> contained in <span class="math inline">\(\hbox{mod}_2\)</span> increase <span class="math inline">\(\mbox{R}^2_2\)</span> compared to <span class="math inline">\(\mbox{R}^2_1\)</span>
</p>
</div>
<p>As a formula, the ratio of the slopes of the model-two segment to the model-one segment is:</p>
<p><span class="math display">\[\Delta \mbox{F} = \frac{n - (^\circ\!{\cal F}_{2} + 1)}{^\circ\!{\cal F}_{2} - ^\circ\!\!{\cal F}_{1}}  \cdot \frac{\hbox{R}^2_{2} - \hbox{R}^2_{1}}{1 - \hbox{R}^2_{2}}\]</span></p>
</div>
<div id="analysis-of-co-variance" class="section level2">
<h2><span class="header-section-number">12.4</span> Analysis of co-variance</h2>
<p>It sometimes happens that <span class="math inline">\(\hbox{mod}_1\)</span> contains all the explanatory variables of direct interest and the point of building <span class="math inline">\(\hbox{mod}_2\)</span> is to include those covariates which ought to be taken into consideration but are not of direct interest. In this situation, the ratio of interest is the slope of the model-one segment to the slope of the residual segment. This is called “analysis of co-variance.”</p>
<p>I like to describe the purpose of including the covariates in <span class="math inline">\(\hbox{mod}2\)</span> as “<em>eating variance</em>.” That is, the covariates can reduce the slope of the residual segment, making it easier for the model-one segment to look steep by comparison.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-means-and-proportions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="remember-inference-isnt-everything.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Compact_inference.pdf", "Compact_inference.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
